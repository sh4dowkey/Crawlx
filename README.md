# CrawlX ğŸ•·ï¸  
*A fast, recursive, and concurrent web crawler written in Go.*

---

## âœ¨ Overview
CrawlX is a lightweight CLI-based web crawler built in Go. It can recursively crawl web pages, follow links, and provide insights into visited URLs.  
The project is structured for **cross-platform usage**, with installation scripts for both Linux/macOS and Windows.  

---

## ğŸš€ Features
- ğŸŒ **Recursive Crawling** â€” explore web pages up to a user-defined depth.  
- âš¡ **Concurrency with Goroutines** â€” crawl multiple links in parallel for faster performance.  
- ğŸ“ **Customizable Flags** â€” configure URL, depth, verbosity.  
- ğŸ“¦ **Cross-Platform Installation** â€” works on Linux/macOS and Windows.  
- ğŸ“Š **Summary Output** â€” keep track of visited links.  

---

## ğŸ“¦ Installation

### Linux / macOS
```bash
git clone https://github.com/sh4dowkey/Crawlx.git
cd Crawlx
chmod +x setup.sh
sudo ./setup.sh
```

### Windows (PowerShell)
```powershell
git clone https://github.com/sh4dowkey/Crawlx.git
cd Crawlx
Set-ExecutionPolicy Bypass -Scope Process -Force
.\setup.ps1
```

Thatâ€™s it! ğŸ‰  
Once installed, you can run `crawlx` from anywhere in your terminal.  

---

## ğŸ› ï¸ Usage

Basic usage:
```bash
crawlx --url https://example.com --depth 2
```

Shorthand flags:
```bash
crawlx -u https://example.com -d 3
```

Verbose mode:
```bash
crawlx -u https://example.com -d 2 --verbose
```

Example output:
```
Crawling: https://example.com
Found: https://example.com/about
Found: https://example.com/contact
Visited: 3 links
```

---

## ğŸ“‚ Project Structure
```
Crawlx/
â”‚â”€â”€ cmd/            # Main source code (main.go, crawl.go)
â”‚â”€â”€ dist/           # Built binaries
â”‚â”€â”€ setup.sh        # Linux/macOS installer
â”‚â”€â”€ setup.ps1       # Windows installer
â”‚â”€â”€ go.mod          # Go module file
â”‚â”€â”€ go.sum          # Go dependencies
```

---

## ğŸ›£ï¸ Roadmap
- [ ] Add support for robots.txt parsing  
- [ ] Export results (JSON/CSV)  
- [ ] Ignore filetypes (e.g., images, PDFs)  
- [ ] Crawl domain restrictions  

---

## ğŸ¤ Contributing
Contributions are welcome!  
1. Fork the repo  
2. Create a new branch (`feature-xyz`)  
3. Commit your changes  
4. Submit a pull request  

---

## ğŸ“œ License
This project is licensed under the **MIT License**.  

---

### ğŸ•¸ï¸ Crawl smarter. Crawl faster. CrawlX.
